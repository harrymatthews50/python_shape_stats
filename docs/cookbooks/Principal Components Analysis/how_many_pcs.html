<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Determining how many principal components to retain &mdash; python_shape_stats 0.1 documentation</title>
      <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/nbsphinx-code-cells.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script src="../../_static/sphinx_highlight.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
        <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            python_shape_stats
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../usage/installation.html">Installing python_shape_stats</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../modules.html">python_shape_stats</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../usage/examples/examples.html">Examples</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">python_shape_stats</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Determining how many principal components to retain</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/cookbooks/Principal Components Analysis/how_many_pcs.ipynb.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="Determining-how-many-principal-components-to-retain">
<h1>Determining how many principal components to retain<a class="headerlink" href="#Determining-how-many-principal-components-to-retain" title="Permalink to this heading"></a></h1>
<p>If you have not already, then check out the example on Principal Components Analysis.</p>
<p>This notebook addresses techniques for identifying the number of ‘real’ principal componenents that represent a data set. Owing to (multi)-collinearity among the features this can be less than the number of PCs it is possible to compute (i.e. whatever is the fewer of the number of training features or the number of observations).</p>
<p>Five methods for this are implemented in python_shape_stats: 1. Heuristic methods 1. Specified variance 2. Scree test 3. Null model-based methods 1. Broken stick 1. Parallel analysis</p>
<p>First up we can simulate some data, which has a known number of true PCs.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre><span></span>from python_shape_stats import helpers
from python_shape_stats.statistical_shape_models import PCA
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import ortho_group
import copy
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre><span></span>##### simulate data with 5 real PCS plus noise
# These are the simulation parameters from the third experiment of Minka (2000) https://vismod.media.mit.edu/tech-reports/TR-514.pdf
TRUE_EIGENVALUES = [10,8,6,4,2]# how many real PCs will there be in
NOISE_VARIANCE = 1/4
NUM_FEATURES =100 # must be greater than len(TRUE_EIGENVALUES)
NUM_OBS  = 60 # how many observations

# make a random number generator to be used throughout
RNG = np.random.default_rng(1546)

#### Generate covariance matrix
# sample a random orthonormal basis (eigenvectors of the covariance matrix)
sampler = ortho_group(dim=NUM_FEATURES,seed=RNG)
pop_eig_vec = sampler.rvs(1)

# put together the true and noise eigenvalues
pop_eig_val = np.concatenate([TRUE_EIGENVALUES,np.tile(NOISE_VARIANCE,NUM_FEATURES-len(TRUE_EIGENVALUES))])

# put together the covariance matrix
pop_cov =pop_eig_vec @ np.diag(pop_eig_val) @ pop_eig_vec.T

# sample data
sample_data = RNG.multivariate_normal(mean=np.zeros(NUM_FEATURES),cov=pop_cov,size=NUM_OBS)
</pre></div>
</div>
</div>
<section id="Heuristic-methods">
<h2>Heuristic methods<a class="headerlink" href="#Heuristic-methods" title="Permalink to this heading"></a></h2>
<section id="Specified-variance">
<h3>Specified variance<a class="headerlink" href="#Specified-variance" title="Permalink to this heading"></a></h3>
<p>Each PC explains a certain amount of variance in the data. The amount of variance explained across all the PCs is sometimes called its ‘spectrum’. The cumulative percentage of the variance explained by PCs 1:i is said to be the cumulative percentage of variance explained at PC i.</p>
<p>One approach is to keep only those pcs that explain up to a specified percentage of the total variation. In my opinion this makes sense if you have some prior knowledge about the ratio of signal to noise in the data. For example in the analysis of variation in 3D anatomy from 3D imaging methods, typically there is very little noise, relative to the amount of biological signal. Therefore common cut-offs used are often relatively high 96%, 98% or 99%. In this context cut-offs of between 90-97%
approximate the results of parallel analysis (see below) on a large dataset (of human <a class="reference external" href="https://doi.org/10.1038%2Fs41588-018-0057-4">faces</a> - see suppl Fig. 2)</p>
<p>You can do this using the ‘trim_perc_var’ method of the PCA class. Below demonstrates this. In this case we trim the model to include only those explaining up to 96% of variation, leaving us with 40 PCs.</p>
<!-- empty raw cell --><div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre><span></span># fit pca to sample data
pca_model = PCA()
pca_model.fit_transform(sample_data)

# plot the eigenvalue spectrum and the cumulative variance explained
fig, (ax1,ax2) = plt.subplots(2,1,sharex=True)
fig.tight_layout(pad = 2.5)
pca_model.scree_plot(ax1)
pca_model.cumulative_variance_plot(ax=ax2)

# keep those PCs that explain up to 96% of the total variance
pca_model.trim_perc_var(96.)

# how many PCs are left
no_pcs = pca_model.n_dim
# how much variance do they explain (it may be slightly more than 96%)
var_exp = pca_model.cumulative_perc_var[-1]

ax2.hlines(var_exp,xmin=ax2.get_xlim()[0],xmax=no_pcs,ls=&#39;:&#39;,colors=&#39;k&#39;)
ax2.vlines(no_pcs,ymin=ax2.get_ylim()[0],ymax=var_exp,ls=&#39;:&#39;,colors=&#39;k&#39;)
<br/></pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
&lt;matplotlib.collections.LineCollection at 0x14448de70&gt;
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/cookbooks_Principal_Components_Analysis_how_many_pcs_7_1.png" src="../../_images/cookbooks_Principal_Components_Analysis_how_many_pcs_7_1.png" />
</div>
</div>
</section>
</section>
<section id="Scree-test">
<h2>Scree test<a class="headerlink" href="#Scree-test" title="Permalink to this heading"></a></h2>
<p>We have already seen a scree plot above. A scree plot simply plots the variance explained (eigenvalue) per PC. The scree test involves visually inspecting such a plot and identifying the point at which the eigenvalues start to level off (this is sometimes called the ‘elbow’ of the plot). The <strong>assumption being that when PCs explain equal amounts of variation the ‘directions’ of the PCs are interchangeable and presumably random.</strong></p>
<p>You can quickly make a scree plot using the ‘scree_plot’ method of the PCA class</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre><span></span># fit pca to sample data
pca_model = PCA()
pca_model.fit_transform(sample_data)
ax,_=pca_model.scree_plot()
print(ax)
# all where I see the approximate elbow
ELBOW_POINT = 6
ax.axvline(ELBOW_POINT,label=&#39;~Elbow&#39;,color=&#39;k&#39;,ls = &#39;:&#39;)
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Axes(0.125,0.11;0.775x0.77)
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
&lt;matplotlib.lines.Line2D at 0x1444db760&gt;
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/cookbooks_Principal_Components_Analysis_how_many_pcs_9_2.png" src="../../_images/cookbooks_Principal_Components_Analysis_how_many_pcs_9_2.png" />
</div>
</div>
<p>With the number of PCs determined you can use the ‘trim_no_pcs’ method of the PCA object to remove the extraneous pcs:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre><span></span>pca_model.trim_no_pcs(ELBOW_POINT)
print(str(pca_model.n_dim)+&#39; PCs retained&#39;)
<br/></pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
6 PCs retained
</pre></div></div>
</div>
<p>Of course this approach is somewhat (possibly a lot) subjective. A further issue is that <strong>completely unstructured data will not actually yield equal eigenvalues</strong> so looking for the point at which eigenvalues approach uniformity may not be the best approach.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre><span></span># shuffle rows and columns independently then do pca
shuff_data = helpers.randomize_matrix(sample_data,seed=RNG)
shuff_mod = PCA()
shuff_mod.fit(shuff_data)

# plot the eigenvalues
ax,_ = shuff_mod.scree_plot()
ax.set_title(&#39;Scree plot\n Unstructured data&#39;)
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Text(0.5, 1.0, &#39;Scree plot\n Unstructured data&#39;)
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/cookbooks_Principal_Components_Analysis_how_many_pcs_13_1.png" src="../../_images/cookbooks_Principal_Components_Analysis_how_many_pcs_13_1.png" />
</div>
</div>
</section>
<section id="Null-model-based-methods">
<h2>Null model-based methods<a class="headerlink" href="#Null-model-based-methods" title="Permalink to this heading"></a></h2>
<p>Although these two methods are somewhat different they conceptualise ‘real’ principal components in the same way: i.e. those PCs those whose eigenvalues differ from what would be expected by random chance (if there was no structure to the data). They both establish what are expected eigenvalue spectra if there was no real structure in the data (I will call this the ‘null’ spectra). Detecting real principal componenents then amounts to detecting where the observed spectrum deviates enought from
the null. The approaches differ in how they determine the null spectra.</p>
<section id="Broken-stick">
<h3>Broken stick<a class="headerlink" href="#Broken-stick" title="Permalink to this heading"></a></h3>
<p>Think of the total variance to be explained as a stick. If you break the stick into N pieces randomly, with a break at any location on the stick being equally likely, some segments will still be longer than others (some components will explain more variance than others). If you repeat this multiple times you will end up with distributions of the expected length of the longest to the shortest stick.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre><span></span># fit pca to sample data
pca_model = PCA()
pca_model.fit_transform(sample_data)

n = pca_model.n_dim# how many pieces is the variation being divided into - equal to the initial number of components
j=np.sum(pca_model.eig_val)# how much variance is being divided up

N_REPS=10
ax = plt.subplot()
for it in range(N_REPS): # estimate N_REPS null spectra and plot
    # randomly select points to break up the variation (the stick)
    bp = RNG.uniform(0, j, n - 1)
    # add the start and end of the stick
    bp = np.concatenate([np.atleast_1d(0),bp,np.atleast_1d(j)])
    # sort ascending
    bp = np.sort(bp)
    # compute length between the i-1th breakpoint to the ith
    segment_lengths = bp[1:]-bp[0:-1]
    # sort descending
    segment_lengths = np.sort(segment_lengths)
    segment_lengths = segment_lengths[::-1]
    ax.plot(np.linspace(1,n,n),segment_lengths,c=&#39;g&#39;,ls=&#39;:&#39;)

ax.set_title(&#39;Null Eigenvalue Spectra\nBroken Stick&#39;)
plt.show()
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/cookbooks_Principal_Components_Analysis_how_many_pcs_17_0.png" src="../../_images/cookbooks_Principal_Components_Analysis_how_many_pcs_17_0.png" />
</div>
</div>
</section>
<section id="Parallel-analysis">
<h3>Parallel analysis<a class="headerlink" href="#Parallel-analysis" title="Permalink to this heading"></a></h3>
<p>Parallel analysis establishes the null spectra slightly differently. Instead it generates multiple datasets of the same size and total variance as the dataset being investigated, but within which there is known to be no structure, and computes the eigenvalue spectra of these datasets.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre><span></span>N_REPS=10
ax = plt.subplot()
for it in range(N_REPS): # estimate N_REPS null spectra and plot
    #shuffle the rows and columns
    shuff_data = helpers.randomize_matrix(sample_data,seed=RNG)
    shuff_pc = PCA()
    shuff_pc.fit(shuff_data,center=False)
    ax.plot(np.linspace(1,n,n),shuff_pc.eig_val,c=&#39;g&#39;,ls=&#39;:&#39;)
ax.set_title(&#39;Null Eigenvalue Spectra\nParallel Analysis&#39;)
plt.show()
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/cookbooks_Principal_Components_Analysis_how_many_pcs_19_0.png" src="../../_images/cookbooks_Principal_Components_Analysis_how_many_pcs_19_0.png" />
</div>
</div>
</section>
<section id="Comparison">
<h3>Comparison<a class="headerlink" href="#Comparison" title="Permalink to this heading"></a></h3>
<p>In practice we do many repetitions to build up these null distributions. These distributions can then be compared to the observed eigenvalue spectrum graphically and by setting a criteria for PC selection. For example we can select up to the last pc whose eigenvalue is above the 50th percentile (median) of the distribution of null spectra.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre><span></span>CI_LEVEL = 95. # plot as filled regions between the 0th and the 95th percentile
THRESHOLD_LEVEL = 50. # accept up to the last PC whose eigenvalue is greater than the 50th percentile of the null spectra
N_REPS = 1000 # how many null spectra to use to build the distribution...in practice set this higher but to quickly illustrate I set this low
# fit pca to sample data
pca_model = PCA()
pca_model.fit_transform(sample_data)

fig,(ax1,ax2) = plt.subplots(2,1,sharey=True)
fig.tight_layout(pad=5.0)
# do broken stick selection
_,n_comps_bs = pca_model.broken_stick_plot(ci_level=CI_LEVEL,threshold_level=THRESHOLD_LEVEL,n_reps=N_REPS,ax=ax1)

# do parallel analysis selection
_,n_comps_pa = pca_model.parallel_analysis_plot(ci_level=CI_LEVEL,threshold_level=THRESHOLD_LEVEL,n_reps=N_REPS,ax=ax2)

print(&#39;Parallel analysis detected &#39;+str(n_comps_pa)+ &#39;PCs.&#39;)
print(&#39;Broken stick detected &#39;+str(n_comps_bs)+ &#39;PCs.&#39;)
plt.show()
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"version_major": 2, "version_minor": 0, "model_id": "54dec4b615304edfa8a39b69d6d57f4d"}</script></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"></pre></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">
</pre></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Parallel analysis detected 4PCs.
Broken stick detected 4PCs.
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/cookbooks_Principal_Components_Analysis_how_many_pcs_21_4.png" src="../../_images/cookbooks_Principal_Components_Analysis_how_many_pcs_21_4.png" />
</div>
</div>
</section>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023, Harold Matthews.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>